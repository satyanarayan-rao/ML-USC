{
    "docs": [
        {
            "location": "/",
            "text": "Machine learning course\n\n\nA total of 5 homeworks. I attempted to put my solutions of the homeworks",
            "title": "Home"
        },
        {
            "location": "/#machine-learning-course",
            "text": "A total of 5 homeworks. I attempted to put my solutions of the homeworks",
            "title": "Machine learning course"
        },
        {
            "location": "/solution_hw_1/",
            "text": "Naive Bayes\n\n\nParametric form of Naive Bayes with Gaussian Assumption\n\n\nFeature vector \nX \\in R^D\nX \\in R^D\n and \nY\nY\n is a binary random vector.\n\n\n\n\nY \\sim Bern(\\pi)\n\n\nY \\sim Bern(\\pi)\n\n\n\n\n\n\nX = \\{X_1, ..., X_D\\}\n\n\nX = \\{X_1, ..., X_D\\}\n\n\n\n\n\n\nP (X_j| Y = y_k ) \\sim N(\\mu_{jk}, \\sigma_j)\n\n\nP (X_j| Y = y_k ) \\sim N(\\mu_{jk}, \\sigma_j)\n\n\n\n\nNaive Bayes assumes the conditional independence of features given \nY\nY\n\n\n\n\nP(X_i = x_i, X_j = x_j | Y = y_k) = P (X_i = x_i | Y =y_k)*P(X_j = x_j| Y= y_k) \\space \\forall i \\neq j\n\n\nP(X_i = x_i, X_j = x_j | Y = y_k) = P (X_i = x_i | Y =y_k)*P(X_j = x_j| Y= y_k) \\space \\forall i \\neq j\n\n\n\n\nHere we have to show that the posterior probability can be written as\nthe posterior of logistic regression.Proceeding with above assumptions.\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{P(X| Y = 1)*P(Y = 1)}{P(X)}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{P(X| Y = 1)*P(Y = 1)}{P(X)}\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP(X) = P(X|Y = 0)*P(Y = 0) + P (X| Y = 1)* P(Y = 1 )\\end{aligned}\n\n\n\\begin{aligned}\nP(X) = P(X|Y = 0)*P(Y = 0) + P (X| Y = 1)* P(Y = 1 )\\end{aligned}\n\n\n\n\nUsing eq. (1.2) in (1.1) and bringing the numerator to denominator\nsimplifies eq. (1) to the following.\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}{1 + \\frac{P(X| Y = 0)*P(Y = 0)}{P(X| Y = 1 )*P(Y = 1)}}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}{1 + \\frac{P(X| Y = 0)*P(Y = 0)}{P(X| Y = 1 )*P(Y = 1)}}\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP(X | Y = y_k) = \\prod_{i=1}^D P (X_i| Y = y_k)\\end{aligned}\n\n\n\\begin{aligned}\nP(X | Y = y_k) = \\prod_{i=1}^D P (X_i| Y = y_k)\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP(X_i | Y = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_i}exp\\left(-\\frac{(X_i - \\mu_{ik})^2}{2\\sigma_{i}^2}\\right)\\end{aligned}\n\n\n\\begin{aligned}\nP(X_i | Y = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_i}exp\\left(-\\frac{(X_i - \\mu_{ik})^2}{2\\sigma_{i}^2}\\right)\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP (Y = 1) = \\pi , P (Y = 0 ) = 1 -\\pi\\end{aligned}\n\n\n\\begin{aligned}\nP (Y = 1) = \\pi , P (Y = 0 ) = 1 -\\pi\\end{aligned}\n\n\n\n\nUsing eq. (1.4),(1.5) and (1.6) in eq. (1.3) we get the simplified form\nas following\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1 - \\pi}\n{\\pi} *\n\\frac{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}    \\right)     }\n{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}    \\right) } }\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1 - \\pi}\n{\\pi} *\n\\frac{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}    \\right)     }\n{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}    \\right) } }\\end{aligned}\n\n\n\n\nSolving further to simpler terms by assuming the following:\n\n\n\n\nZ_{i0} = \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}\n\n\nZ_{i0} = \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}\n\n\n\n\n\n\nZ_{i1} = \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}\n\n\nZ_{i1} = \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}\n\n\n\n\n\n\n\\begin{aligned}\n Z_{i1} - Z_{i0}  = \\frac{(2X_i - (\\mu_{i0} + \\mu_{i1}) )*(\\mu_{i0} - \\mu_{i1})}{2\\sigma_i^2}\n \\end{aligned}\n\n\n\\begin{aligned}\n Z_{i1} - Z_{i0}  = \\frac{(2X_i - (\\mu_{i0} + \\mu_{i1}) )*(\\mu_{i0} - \\mu_{i1})}{2\\sigma_i^2}\n \\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n= \\frac{X_i*(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}  - \\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}{2\\sigma_i^2}\\end{aligned}\n\n\n\\begin{aligned}\n= \\frac{X_i*(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}  - \\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}{2\\sigma_i^2}\\end{aligned}\n\n\n\n\neq. (1.7) can be rewritten using simplified terms like \nZ_{i1} - Z_{i0}\nZ_{i1} - Z_{i0}\n\nas following.\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1-\\pi}\n{\\pi}*\\prod_{i=1}^{D}exp \\left( \\frac{-(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2} + \\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i   \\right)}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1-\\pi}\n{\\pi}*\\prod_{i=1}^{D}exp \\left( \\frac{-(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2} + \\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i   \\right)}\\end{aligned}\n\n\n\n\nProduct term in denominator of eq. (1.8) can be substituted with summand\ninside the exponential expression, and \n\\frac{1-\\pi}{\\pi}\n\\frac{1-\\pi}{\\pi}\n can be\nwritten as \nexp\\left(-log\\frac{\\pi}{1 - \\pi}\\right).\nexp\\left(-log\\frac{\\pi}{1 - \\pi}\\right).\n Doing above\nchanges to eq. (1.8) it reduces to the following form.\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + exp \\left\\lbrace - \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i\n\\right\\rbrace}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + exp \\left\\lbrace - \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i\n\\right\\rbrace}\\end{aligned}\n\n\n\n\nWe can see clearly in eq. (1.9) that the posterior probability is taking\nthe form of logistic regression. Comparing with the logistic regression\nexpression. We can write the parameters as follows:\n\n\n\n\n\\begin{aligned}\n\\omega_0 = \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\\end{aligned}\n\n\n\\begin{aligned}\n\\omega_0 = \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\textbf{w} = \\left[\\frac{(\\mu_{00} - \\mu_{01})}{\\sigma_1^2} , ... , \\frac{(\\mu_{D0} - \\mu_{D1})}{\\sigma_D^2}\\right]^T\\end{aligned}\n\n\n\\begin{aligned}\n\\textbf{w} = \\left[\\frac{(\\mu_{00} - \\mu_{01})}{\\sigma_1^2} , ... , \\frac{(\\mu_{D0} - \\mu_{D1})}{\\sigma_D^2}\\right]^T\\end{aligned}\n\n\n\n\nParametric estimation for Naive Bayes with Gaussian assumption\n\n\nWe have training set of size \nN\nN\n of the form\n\n(\\textbf{x}_\\textbf{i}, y_i)\n(\\textbf{x}_\\textbf{i}, y_i)\n, where\n\n\n\n\n\\begin{aligned}\n\\textbf{x}_\\textbf{i} = (x_{i1}, ..., x_{iD})^T, y_i \\in \\{0,1\\}\\end{aligned}\n\n\n\\begin{aligned}\n\\textbf{x}_\\textbf{i} = (x_{i1}, ..., x_{iD})^T, y_i \\in \\{0,1\\}\\end{aligned}\n\n\n\n\nSince we have to estimate the distribution parameters such as\n\n\\pi_i, \\mu_{jk},\n\\pi_i, \\mu_{jk},\n and \n\\sigma_j\n\\sigma_j\n. We can do this by writing log\nlikelihood and differentiating w.r.t corresponding parameters and set to\nzero to get the values. Following derivation are for the log likelihood\nand parameter estimation.\n\n\nJoint probability can be written as\n\n\n\n\n\\begin{aligned}\nP(X, Y ) = \\prod_{i = 1 }^{N} P (X_i, Y_i)= \\prod_{i = 1 }^{N} P(X_i | Y_i = y_k)*P (Y_i = y_k)\\end{aligned}\n\n\n\\begin{aligned}\nP(X, Y ) = \\prod_{i = 1 }^{N} P (X_i, Y_i)= \\prod_{i = 1 }^{N} P(X_i | Y_i = y_k)*P (Y_i = y_k)\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n= \\prod_{i = 1}^{N}P(Y_i = y_k)\\prod_{j = 1}^{D}P(X_{ij} = x_{ij} | Y_i = y_k)\\end{aligned}\n\n\n\\begin{aligned}\n= \\prod_{i = 1}^{N}P(Y_i = y_k)\\prod_{j = 1}^{D}P(X_{ij} = x_{ij} | Y_i = y_k)\\end{aligned}\n\n\n\n\nIn above deduction we have used the naive bayes assumption. Now we know\nthe following:\n\n\n\n\n\\begin{aligned}\nP(Y_i = y_k) = \\pi^{y_k}*(1-\\pi)^{(1-y_k)}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y_i = y_k) = \\pi^{y_k}*(1-\\pi)^{(1-y_k)}\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP(X_{ij} = x_{ij} | Y_i = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_j}exp\\left(-\\frac{(x_{ij} - \\mu_{jk})^2}{2\\sigma_{j}^2}\\right)\\end{aligned}\n\n\n\\begin{aligned}\nP(X_{ij} = x_{ij} | Y_i = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_j}exp\\left(-\\frac{(x_{ij} - \\mu_{jk})^2}{2\\sigma_{j}^2}\\right)\\end{aligned}\n\n\n\n\nSince we know that log likelihood is the log of joint pdf. We can take\nthe log on both side of eq. (1.10) to get the log likelihood.\n\n\n\n\n\\begin{aligned}\nl(\\theta | X, Y) = logP(X,Y) = \\sum_{i = 1}^{N}\\left( log P(Y_i = y_k) + \\sum_{j = 1}^{D} log P (X_{ij} = x_{ij} | Y_i = y_k)\n\\right)  \\end{aligned}\n\n\n\\begin{aligned}\nl(\\theta | X, Y) = logP(X,Y) = \\sum_{i = 1}^{N}\\left( log P(Y_i = y_k) + \\sum_{j = 1}^{D} log P (X_{ij} = x_{ij} | Y_i = y_k)\n\\right)  \\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n= \\sum_{i = 1}^{N} \\left( y_klog\\pi + (1-y_k)log(1 - \\pi) + \\sum_{j = 1}^{D} \\left( -log ( (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{jk})^2}\n{2\\sigma_j^2}\n\\right)\n\\right)\\end{aligned}\n\n\n\\begin{aligned}\n= \\sum_{i = 1}^{N} \\left( y_klog\\pi + (1-y_k)log(1 - \\pi) + \\sum_{j = 1}^{D} \\left( -log ( (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{jk})^2}\n{2\\sigma_j^2}\n\\right)\n\\right)\\end{aligned}\n\n\n\n\nSuppose there are m data points with label 1 and N-m with label 0, above\nequation can be rewritten as following:\n\n\n\n\n\\begin{aligned}\nl(\\theta | X, Y) = \\sum_{i:Y_i = y_k = 1}y_klog\\pi  + \\sum_{i:Y_i = y_k = 0}(1-y_k)log(1-\\pi) \\\\ + \\sum_{i:Y_i = y_k = 1}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j1})^2}{2\\sigma} \n\\right) \\\\ +  \\sum_{i:Y_i = y_k = 0}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j0})^2}{2\\sigma} \n\\right)\\end{aligned}\n\n\n\\begin{aligned}\nl(\\theta | X, Y) = \\sum_{i:Y_i = y_k = 1}y_klog\\pi  + \\sum_{i:Y_i = y_k = 0}(1-y_k)log(1-\\pi) \\\\ + \\sum_{i:Y_i = y_k = 1}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j1})^2}{2\\sigma} \n\\right) \\\\ +  \\sum_{i:Y_i = y_k = 0}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j0})^2}{2\\sigma} \n\\right)\\end{aligned}\n\n\n\n\nCalculating MLE for \n\\pi\n\\pi\n:\n\n\n\n\n\\begin{aligned}\n\\hat{\\pi} = \\left\\lbrace \\pi: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = 0  \\right\\rbrace\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\pi} = \\left\\lbrace \\pi: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = 0  \\right\\rbrace\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = \\frac{m}{\\pi} - \\frac{(N - m)}{1 - \\pi} + 0 + 0  \\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = \\frac{m}{\\pi} - \\frac{(N - m)}{1 - \\pi} + 0 + 0  \\end{aligned}\n\n\n\n\nEquating the above equation to zero and solving for \n\\pi\n\\pi\n\n\n\n\n\\begin{aligned}\nm(1 - \\pi)  - (N-m)\\pi = 0 \n\\implies m - N\\pi = 0 \n\\implies \\pi = \\frac{m}{N} \\end{aligned}\n\n\n\\begin{aligned}\nm(1 - \\pi)  - (N-m)\\pi = 0 \n\\implies m - N\\pi = 0 \n\\implies \\pi = \\frac{m}{N} \\end{aligned}\n\n\n\n\nSo the likelihood parameter estimation of \n\\pi\n\\pi\n is:\n\n\n\n\n\\begin{aligned}\n\\hat{\\pi} = \\frac{m}{N} = \\frac{\\#label = 1 }{N}\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\pi} = \\frac{m}{N} = \\frac{\\#label = 1 }{N}\\end{aligned}\n\n\n\n\nLikelihood for \n\\mu_{jk}\n\\mu_{jk}\n:\n\n\n\n\n\\begin{aligned}\n\\hat{\\mu}_{jk} = \\left\\lbrace \\mu_{jk}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\mu_{jk}} = 0  \\right\\rbrace\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\mu}_{jk} = \\left\\lbrace \\mu_{jk}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\mu_{jk}} = 0  \\right\\rbrace\\end{aligned}\n\n\n\n\nWe can do this in two parts, k = 0 and k = 1 for k = 0, we shall only\nhave the fourth term in partial differentiation, other term will give\nzeros.\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi}  = \\sum_{i:Y_i = y_k = 0}- \\frac{(x_{ij} - \\mu_{jo})}{\\sigma_j^2}*(-1) = 0 \\\\\n\\implies \\sum_{i:Y_i = y_k = 0}(x_{ij} - \\mu_{jo}) = 0 \\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi}  = \\sum_{i:Y_i = y_k = 0}- \\frac{(x_{ij} - \\mu_{jo})}{\\sigma_j^2}*(-1) = 0 \\\\\n\\implies \\sum_{i:Y_i = y_k = 0}(x_{ij} - \\mu_{jo}) = 0 \\end{aligned}\n\n\n\n\nFrom above assumption we know the count of zero labels are \nN-m\nN-m\n Solving\nabove equality gives us the following\n\n\n\n\n\\begin{aligned}\n(N-m)*\\mu_{j0} = \\sum_{i:Y_i = y_k = 0} x_{ij}\\end{aligned}\n\n\n\\begin{aligned}\n(N-m)*\\mu_{j0} = \\sum_{i:Y_i = y_k = 0} x_{ij}\\end{aligned}\n\n\n\n\nwhich gives us\n\n\n\n\n\\begin{aligned}\n\\hat{\\mu}_{j0} = \\frac{\\sum_{i:Y_i = y_k = 0} x_{ij}}{(N-m)}\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\mu}_{j0} = \\frac{\\sum_{i:Y_i = y_k = 0} x_{ij}}{(N-m)}\\end{aligned}\n\n\n\n\nSimilarly we get the estimate for \n\\mu_{j1}\n\\mu_{j1}\n\n\n\n\n\\begin{aligned}\n\\hat{\\mu}_{j1} = \\frac{\\sum_{i:Y_i = y_k = 1} x_{ij}}{(m)}\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\mu}_{j1} = \\frac{\\sum_{i:Y_i = y_k = 1} x_{ij}}{(m)}\\end{aligned}\n\n\n\n\nLikelihood for \n\\sigma_{j}\n\\sigma_{j}\n:\n\n\n\n\n\\begin{aligned}\n\\hat{\\sigma}_{j} = \\left\\lbrace \\sigma_{j}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}} = 0  \\right\\rbrace\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{\\sigma}_{j} = \\left\\lbrace \\sigma_{j}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}} = 0  \\right\\rbrace\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}}  = \\sum_{i:Y_i =y_k = 1} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j1})^2}{\\sigma_{j}^3}\n\\right)\\\\\n+ \\sum_{i:Y_i = y_k = 0} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j0})^2}{\\sigma_{j}^3}\n\\right)\\\\\n= - \\frac{N}{\\sigma_{j}} + \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{\\sigma_{j}^3}\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}}  = \\sum_{i:Y_i =y_k = 1} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j1})^2}{\\sigma_{j}^3}\n\\right)\\\\\n+ \\sum_{i:Y_i = y_k = 0} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j0})^2}{\\sigma_{j}^3}\n\\right)\\\\\n= - \\frac{N}{\\sigma_{j}} + \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{\\sigma_{j}^3}\\end{aligned}\n\n\n\n\nEquating it to zero will give the following:\n\n\n\n\n\\begin{aligned}\n\\sigma_{j}^2 = \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}\\\\\n\\implies \\hat{\\sigma}_{j} = \\sqrt{\\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}}\\end{aligned}\n\n\n\\begin{aligned}\n\\sigma_{j}^2 = \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}\\\\\n\\implies \\hat{\\sigma}_{j} = \\sqrt{\\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}}\\end{aligned}\n\n\n\n\nNearest Neighbor\n\n\nFeature weighting in low dimension\n\n\nFeature weighting is required in case of noise, because noise affect the\neuclidean distance measures which can lead to poor performance of KNN.\nThis is also known as Achilles\u2019 heel of KNN. In case of low dimension\n\nD = 3\nD = 3\n with discretized values of weights in each dimension. We can do\ngrid search to look for the optimal weights which can give the best\ntraining accuracy. The procedure explained below can be used to find the\noptimal weights for the features. Here we have following assumption\n\n\n\n\n\\begin{aligned}\nX_i = \\left(x_{i1}, x_{i2}, x_{i3}\\right)^T\\\\\nW = \\left(w_1, w_2, w_3\\right)\\\\\nw_i \\in \\{w_{i1}, w_{i2}, .,.,., w_{ik}\\}\\end{aligned}\n\n\n\\begin{aligned}\nX_i = \\left(x_{i1}, x_{i2}, x_{i3}\\right)^T\\\\\nW = \\left(w_1, w_2, w_3\\right)\\\\\nw_i \\in \\{w_{i1}, w_{i2}, .,.,., w_{ik}\\}\\end{aligned}\n\n\n\n\nThe weights has been discretized to \nk\nk\n values. We can just do grid\nsearch by using following pseudo code. opt (W) = {} for\n\nw_1 = w_{11}:w_{1k}\nw_1 = w_{11}:w_{1k}\n\n\nfor \nw_2 = w_{21}:w_{2k}\nw_2 = w_{21}:w_{2k}\n\n\nfor \nw_3 = w_{31}:w_{3k}\nw_3 = w_{31}:w_{3k}\n\n\nfor \ni = 1:N\ni = 1:N\n\n\nfor \nj\\neq i = 1:N\nj\\neq i = 1:N\n\n\ncompute \nd(X_i, X_j)\nd(X_i, X_j)\n\n\nend\n\n\nend\n\n\nCompute the training accuracy and update opt(W) if accuracy is\n\n\nbetter.\n\n\nend\n\n\nend\n\n\nend\n\n\nFor this approach we can also use dynamic programming to reduce the\ncomputation, we can maintain the \nN\nN\nx\nN\nN\nx\nD\nD\n matrix with entries of\neuclidean distances of feature vectors. Afterwards we can use this\nmatrix as lookup table and just multiply with the weights to get the\nweighted feature distance. Using the Dynamic programming approach will\nlead to following complexity: Time complexity:\n\n\n\n\nO(k^3)\n\n\nO(k^3)\n\n\n\n\nSpace complexity:\n\n\n\n\nO(N^2)\n\n\nO(N^2)\n\n\n\n\nFeature weighting in higher dimensions\n\n\nIn higher dimension above approach of finding optimal weight will lead\nto exponential time complexity \nO(k^D)\nO(k^D)\n. This is curse of\ndimensionality. As being instructed by TA that we do not have to use any\nstatistical approach to construct the method for assigning weights to\nfeatures (We have to design algorithm). If we present the weighted\ndistances calculation in matrix form, it will appear as follows:\n\n\n\n\n\\begin{aligned}\n\\hat{Label}(X_1) = Label \\left(X_j : j = \\min_{j}\n\\begin{bmatrix}\nw_1 &  . &  w_D\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_{11} - x_{21})^2 &  (x_{11} - x_{31})^2 &  . & (x_{11} - x_{N1})^2 \\\\\n(x_{12} - x_{22})^2  & (x_{12} - x_{32})^2 &  . & (x_{12} - x_{N2})^2 \\\\\n.  & . & .  & . & \\\\\n.  & . & .  & . & \\\\\n(x_{1D} - x_{2D})^2 & (x_{1D} - x_{3D})^2  &  . & (x_{1D} - x_{ND})^2\n\\end{bmatrix}\n\\right)\\end{aligned}\n\n\n\\begin{aligned}\n\\hat{Label}(X_1) = Label \\left(X_j : j = \\min_{j}\n\\begin{bmatrix}\nw_1 &  . &  w_D\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_{11} - x_{21})^2 &  (x_{11} - x_{31})^2 &  . & (x_{11} - x_{N1})^2 \\\\\n(x_{12} - x_{22})^2  & (x_{12} - x_{32})^2 &  . & (x_{12} - x_{N2})^2 \\\\\n.  & . & .  & . & \\\\\n.  & . & .  & . & \\\\\n(x_{1D} - x_{2D})^2 & (x_{1D} - x_{3D})^2  &  . & (x_{1D} - x_{ND})^2\n\\end{bmatrix}\n\\right)\\end{aligned}\n\n\n\n\nWe do above calculation for every data point. Since there are \nk^D\nk^D\n\npossible matrices \n[w_1 ... w_D ]\n[w_1 ... w_D ]\n, we can not calculate this in\npolynomial time. One way to select meaningful features from the haystack\nis to calculate the training accuracy on each individual feature by\nsetting other\u2019s weight to zero. we assign the weight to be training\naccuracy of that individual feature. We can decide how many feature we\nhave to take into consideration. This will result into the limited\nfeatures with weight assigned on each of them. \n2\nnd\n method\n Objective\nfunction for above problem will look like following:\n\n\n\n\n\\begin{aligned}\nJ(W) = \\sum_{i=1}^{N}\\left( \\hat{Label}(X_i) - Label (X_i) \\right)\\end{aligned}\n\n\n\\begin{aligned}\nJ(W) = \\sum_{i=1}^{N}\\left( \\hat{Label}(X_i) - Label (X_i) \\right)\\end{aligned}\n\n\n\n\nThe characteristic of above objective function is a curve in higher\ndimension with multiple local minimum. We can use the gradient descent\napproach to find the local minima\u2019s we can start with different set of\nrandom weight vectors and try to find the different minimum, select the\nbest one out of these and return those weight vectors as weighted\nfeature vector. This is a polynomial time algorithm, but of course this\nmay not give the optimal solution.\n\n\nLogistic regression\n\n\nNegative log likelihood or Loss function\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right)\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right)\\end{aligned}\n\n\n\n\nWe know that\n\n\n\n\n\\begin{aligned}\nP (Y = 1 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})} = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}\n\n\n\\begin{aligned}\nP (Y = 1 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})} = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP (Y = 0 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) =  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}\n\n\n\\begin{aligned}\nP (Y = 0 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) =  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nP (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})^{y_i} (  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )^{1-y_i}\\\\\n\\log P (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} )  = y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )\\end{aligned}\n\n\n\\begin{aligned}\nP (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})^{y_i} (  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )^{1-y_i}\\\\\n\\log P (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} )  = y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )\\end{aligned}\n\n\n\n\nWriting the negative log likelihood in simpler form using above\nexpression\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\log(P (Y = y_i| \\textbf{X} = \\textbf{x}_{\\textbf{i}}))\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\log(P (Y = y_i| \\textbf{X} = \\textbf{x}_{\\textbf{i}}))\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\left(y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) ) \n\\right)\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\left(y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) ) \n\\right)\\end{aligned}\n\n\n\n\nProof for convexity of loss function\n\n\nTo prove the convexity we are going to show that the hessian matrix will\nbe positive semidefinite. In order to make the derivation simpler we are\ngoing to use the property of convex functions. If \nf(x)\nf(x)\n and \ng(x)\ng(x)\n are\ntwo convex function then their sum \nh(x)\nh(x)\n is also going to be the convex\nfunction Proof: As we know from the property of convex function\n\n\n\n\nf(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\n\nf(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2)\n\n\n\n\n\n\ng(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda g(x_1) + (1 - \\lambda)g(x_2)\n\n\ng(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda g(x_1) + (1 - \\lambda)g(x_2)\n\n\n\n\nAdding above two equations we get the following:\n\n\n\n\nf(\\lambda x_1 + (1-\\lambda)x_2 ) + g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2) + \\lambda g(x_1) + (1 - \\lambda)g(x_2)\n\n\nf(\\lambda x_1 + (1-\\lambda)x_2 ) + g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2) + \\lambda g(x_1) + (1 - \\lambda)g(x_2)\n\n\n\n\n\n\nh(\\lambda x_1 + (1-\\lambda)x_2 )) <= \\lambda h(x_1) + (1 - \\lambda)h(x_2)\n\n\nh(\\lambda x_1 + (1-\\lambda)x_2 )) <= \\lambda h(x_1) + (1 - \\lambda)h(x_2)\n\n\n\n\nSo \nh(x)\nh(x)\n is also convex. Multiplying by any positive constant preserves\nthe convexity. Considering just one term from loss function.\n\n\n\n\n\\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\end{aligned}\n\n\n\\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i)\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i)\\end{aligned}\n\n\n\n\nIf we prove that $T(\\textbf{w,x},y) $ is convex then we can say that\n\n\\mathcal{L}(\\textbf{w})\n\\mathcal{L}(\\textbf{w})\n is convex (by above proved lemma).\n\n\n\n\n\\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\\\\n= -y\\log \\left(\\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) - (1-y)\\log\\left(1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n=y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y)\\log\\left(\\frac{\\exp(-\\textbf{w}^{T}\\textbf{x})}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n= y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y) \\left( -\\textbf{w}^{T}\\textbf{x} - \\log\\left(  1 + \\exp(-\\textbf{w}^{T}\\textbf{x}\n\\right)\n\\right)\\\\\n=(1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\\end{aligned}\n\n\n\\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\\\\n= -y\\log \\left(\\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) - (1-y)\\log\\left(1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n=y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y)\\log\\left(\\frac{\\exp(-\\textbf{w}^{T}\\textbf{x})}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n= y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y) \\left( -\\textbf{w}^{T}\\textbf{x} - \\log\\left(  1 + \\exp(-\\textbf{w}^{T}\\textbf{x}\n\\right)\n\\right)\\\\\n=(1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\\end{aligned}\n\n\n\n\nFinally we get:\n\n\n\n\nT(\\textbf{w,x},y) = (1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\n\n\nT(\\textbf{w,x},y) = (1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\n\n\n\n\nWe need to find the hessian\n(H)\n(H)\n of this which is basically \nD\nD\nx\nD\nD\n\nmatrix and the element \nH(j,k)\nH(j,k)\n is:\n\n\n\n\n\\begin{aligned}\nH(j,k) = \\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k}\\end{aligned}\n\n\n\\begin{aligned}\nH(j,k) = \\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k}\\end{aligned}\n\n\n\n\nNow calculating the \nH(j,k)\nH(j,k)\n. Calculating the first derivative.\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial T(\\textbf{w,x},y)}{\\partial w_j} = \\frac{-x_j * \\exp(-\\textbf{w}^{T}\\textbf{x})}{ 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})} + x_j(1-y)\\\\\n= -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) +x_j(1-y)\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial T(\\textbf{w,x},y)}{\\partial w_j} = \\frac{-x_j * \\exp(-\\textbf{w}^{T}\\textbf{x})}{ 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})} + x_j(1-y)\\\\\n= -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) +x_j(1-y)\\end{aligned}\n\n\n\n\nDifferentiating above w.r.t. \nw_k\nw_k\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = 0 + x_j \\left[ \\frac{(-1)*\\exp(-\\textbf{w}^{T}\\textbf{x})*(-x_k) }{\\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)^2}\n\\right] + 0 \\\\\n= x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = 0 + x_j \\left[ \\frac{(-1)*\\exp(-\\textbf{w}^{T}\\textbf{x})*(-x_k) }{\\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)^2}\n\\right] + 0 \\\\\n= x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k \\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k \\end{aligned}\n\n\n\n\nNote: \n\\sigma(x) >= 0 \\hspace{8pt} \\forall x\n\\sigma(x) >= 0 \\hspace{8pt} \\forall x\n\n\nIf we fill out the hessian matrix it will look like following:\n\n\n\n\n\\begin{aligned}\nH = \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x})) \\begin{bmatrix}\nx_1^2 & x_1x_2 &  . & .& . & x_1x_D\\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\nx_Dx_1 & x_Dx_2 & . & . & . & x_D^2\n\\end{bmatrix}\\end{aligned}\n\n\n\\begin{aligned}\nH = \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x})) \\begin{bmatrix}\nx_1^2 & x_1x_2 &  . & .& . & x_1x_D\\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\nx_Dx_1 & x_Dx_2 & . & . & . & x_D^2\n\\end{bmatrix}\\end{aligned}\n\n\n\n\nLet\n$ K =  \\sigma(\\textbf{w}\n{T}\\textbf{x})*(1-\\sigma(\\textbf{w}\n{T}\\textbf{x}))$\nwhich is always greater than or equal to zero, matrix in above\nexpression can be decomposed into \nXX^T\nXX^T\n where X is a column matrix. So\n\nH\nH\n can be rewritten as\n\n\n\n\n\\begin{aligned}\nH = K XX^T\\end{aligned}\n\n\n\\begin{aligned}\nH = K XX^T\\end{aligned}\n\n\n\n\nwhere\n\n\n\n\nX = \\begin{bmatrix}\nx_1 & x_2 & . & . & .& . x_D\n\\end{bmatrix}^T\n\n\nX = \\begin{bmatrix}\nx_1 & x_2 & . & . & .& . x_D\n\\end{bmatrix}^T\n\n\n\n\nH\nH\n is a positive semidefinite matrix because if we take any vector \nV\nV\n\nand calculate \nV^THV\nV^THV\n then it is always $>= 0 $. Following is the proof.\n\n\n\n\n\\begin{aligned}\nV^THV = V^T KXX^T V = K (X^TV)^T (X^TV) = K ||X^TV||^2 >=0 \\hspace{20pt} \\end{aligned}\n\n\n\\begin{aligned}\nV^THV = V^T KXX^T V = K (X^TV)^T (X^TV) = K ||X^TV||^2 >=0 \\hspace{20pt} \\end{aligned}\n\n\n\n\nSince the hessian is PSD we can say that function is convex. Referring\nto eq.(3.2) the loss function is just the linear combination of \nT(.)\nT(.)\n.\nAs we proved in the lemma that sum of convex functions is a convex\nfunction. Hence it is proved that the loss function is convex.\n\n\nMagnitude of optimal w\n\n\nConsidering the binary logistic regression and samples are linearly\nseparable.\n\n\n\n\nWhen sample points are linearly separable then the features determine\nthe label deterministically, it implies that given the feature vector\n(with true label 1) the regression model should predict the label with\nprobability close to 1, and if the feature vector belongs to label 0\nthen model should predict the probability close to zero. Lets look at\nthe logistic regression model\n\n\n\n\n\\begin{aligned}\nP(Y = 1 | X) = \\sigma(\\textbf{w}^{T}\\textbf{x}) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\\end{aligned}\n\n\n\\begin{aligned}\nP(Y = 1 | X) = \\sigma(\\textbf{w}^{T}\\textbf{x}) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\\end{aligned}\n\n\n\n\nFor linearly separable data \nP (Y = 1 | X)\nP (Y = 1 | X)\n should either be 0 or 1 in\nprinciple. This implies that \n\\textbf{w}^{T}\\textbf{x}\n\\textbf{w}^{T}\\textbf{x}\n should either\ngoes to \n-\\infty\n-\\infty\n or \n\\infty\n\\infty\n irrespective of \n\\textbf{x}\n\\textbf{x}\n\n\n\n\n\\begin{aligned}\n\\textbf{w}^{T}\\textbf{x} \\longrightarrow \\infty \\hspace{14pt} \\text{when \\textbf{x} determines Y = 1}\\end{aligned}\n\n\n\\begin{aligned}\n\\textbf{w}^{T}\\textbf{x} \\longrightarrow \\infty \\hspace{14pt} \\text{when \\textbf{x} determines Y = 1}\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\textbf{w}^{T}\\textbf{x} \\longrightarrow -\\infty \\hspace{14pt} \\text{when \\textbf{x} determines Y = 0}\\end{aligned}\n\n\n\\begin{aligned}\n\\textbf{w}^{T}\\textbf{x} \\longrightarrow -\\infty \\hspace{14pt} \\text{when \\textbf{x} determines Y = 0}\\end{aligned}\n\n\n\n\nboth of above statement implies for perfect Logistic regression\nclassifier \n||\\textbf{w}||^2\n||\\textbf{w}||^2\n should tend to infinity for above two\nstatements to be valid. Following figure can explain the behavior of\n\n\\sigma (.)\n\\sigma (.)\n on different values of w.\n\n\n\n\n\n\n\n\nAt high value of w (Fig.3), we can see the strong confidence of taking\nthe decision (no value of \n\\sigma(.)\n\\sigma(.)\n lying between 0 and 1).\n\n\nRegularized logistic regression\n\n\nSince optimal \n\\textbf{w} \\longrightarrow \\infty\n\\textbf{w} \\longrightarrow \\infty\n, in order to handle\nthe numerical instability the regularization term is being added to the\nloss function, and the regularized linear regression looks like\nfollowing:\n\n\n\n\n\\begin{aligned}\n\\mathcal{L} (\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right) + \\lambda||\\textbf{w}||_2^2\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L} (\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right) + \\lambda||\\textbf{w}||_2^2\\end{aligned}\n\n\n\n\nComputing gradient w.r.t. \n\\omega_i\n\\omega_i\n. Referring to eq. (3.2) and the\nfirst derivative of \nT(\\textbf{w,x},y)\nT(\\textbf{w,x},y)\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) +  \\lambda||\\textbf{w}||_2^2\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) +  \\lambda||\\textbf{w}||_2^2\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =  \\sum_{i = 1 }^{n} \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} + 2\\lambda \\omega_j\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =  \\sum_{i = 1 }^{n} \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} + 2\\lambda \\omega_j\\end{aligned}\n\n\n\n\nWe have already calculated the term\n\n\\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j}\n\\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j}\n\nin the process of proving the hessian matrix to be PSD. We can just\nplugin that term here. After plugging in the term we get the following :\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right) +x_j(1-y_i)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right) +x_j(1-y_i)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\n\nFurther simplifying above equation( \nx_j\nx_j\n cancels out ).\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( y_i - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right)\n\\right)\n+ 2\\lambda\\omega_j \\\\\n= -\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( y_i - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right)\n\\right)\n+ 2\\lambda\\omega_j \\\\\n= -\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =\n-\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =\n-\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}\n\n\n\n\nUnique solution of the regularized loss function\n\n\nSince we have already proven that the loss function without\nregularization is convex and the regularization term is also convex\n(because \n\\lambda > 0\n\\lambda > 0\n and \n||\\textbf{w}||^2 > 0\n||\\textbf{w}||^2 > 0\n) so the linear\ncombination will always be convex. There will be \n\\textbf{w}^*\n\\textbf{w}^*\n for\nwhich\n\n\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}^*) <\\mathcal{L}(\\textbf{w}) \\hspace{10pt} \\forall \\textbf{w}\\neq \\textbf{w}^*\\in \\textbf{W}\\end{aligned}\n\n\n\\begin{aligned}\n\\mathcal{L}(\\textbf{w}^*) <\\mathcal{L}(\\textbf{w}) \\hspace{10pt} \\forall \\textbf{w}\\neq \\textbf{w}^*\\in \\textbf{W}\\end{aligned}\n\n\n\n\nWe can start at any \n\\textbf{w}_0\n\\textbf{w}_0\n and reach \n\\textbf{w}^*\n\\textbf{w}^*\n by using\ngradient descent approach.\n\n\nDecision Tree\n\n\nBuilding a decision tree\n\n\nSelection of predictor to form the decision tree\n We shall be\nchoosing the predictor which can maximize the information gain.\n\n\n\n\n\\begin{aligned}\nIG(Predictor) = H(Target) - H(Target|Predictor)\\\\\\end{aligned}\n\n\n\\begin{aligned}\nIG(Predictor) = H(Target) - H(Target|Predictor)\\\\\\end{aligned}\n\n\n\n\nWhere \nH(Target)\nH(Target)\n is the entropy of target and \nH(Target|Predictor)\nH(Target|Predictor)\n is\nconditional entropy over predictor which is basically weighted sum of\nentropies of each branch after splitting using the predictor Here we\nhave\n\n\n\n\n\\begin{aligned}\nPredictor \\in \\{Weather, Traffic\\}\n,\\hspace{10pt}Taget \\in \\{Accident\\hspace{3pt}rate\\}\\end{aligned}\n\n\n\\begin{aligned}\nPredictor \\in \\{Weather, Traffic\\}\n,\\hspace{10pt}Taget \\in \\{Accident\\hspace{3pt}rate\\}\\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nIG(Weather) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Weather)\\\\\nIG(Traffic) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Traffic)\\end{aligned}\n\n\n\\begin{aligned}\nIG(Weather) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Weather)\\\\\nIG(Traffic) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Traffic)\\end{aligned}\n\n\n\n\nLooking at eq. (4.1) and (4.2) we can say that the predictor which gives\nminimum conditional entropy will lead to maximum information gain. So we\njust calculate the conditional entropy and decide the predictor to do\nthe splitting in order to form decision tree.\n\n\nCase 1:\n Choosing \nWeather\nWeather\n as root for sunny branch the uncertainty\nwill be following:\n\n\n\n\n\\begin{aligned}\n-\\left(\\frac{23}{28}\\log_2\\frac{23}{28} + \\frac{5}{28}\\log_2\\frac{5}{28}\n\\right) = 0.6769\\end{aligned}\n\n\n\\begin{aligned}\n-\\left(\\frac{23}{28}\\log_2\\frac{23}{28} + \\frac{5}{28}\\log_2\\frac{5}{28}\n\\right) = 0.6769\\end{aligned}\n\n\n\n\nfor rainy branch the uncertainty will be following:\n\n\n\n\n\\begin{aligned}\n-\\left(\\frac{50}{72}\\log_2\\frac{50}{72} + \\frac{22}{72}\\log_2\\frac{22}{72}\n\\right) = 0.8880\\end{aligned}\n\n\n\\begin{aligned}\n-\\left(\\frac{50}{72}\\log_2\\frac{50}{72} + \\frac{22}{72}\\log_2\\frac{22}{72}\n\\right) = 0.8880\\end{aligned}\n\n\n\n\nthe condition entropy would be the weighted average over the branches:\n\n\n\n\n\\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{28}{100}*0.6769 + \\frac{72}{100}*0.8880 =  0.8289\\end{aligned}\n\n\n\\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{28}{100}*0.6769 + \\frac{72}{100}*0.8880 =  0.8289\\end{aligned}\n\n\n\n\nCase 2:\n Choosing \nTraffic\nTraffic\n as root for heavy branch the uncertainty\nwill be following:\n\n\n\n\n\\begin{aligned}\n-\\left(\\frac{73}{73}\\log_2\\frac{73}{73} + \\frac{0}{73}\\log_2\\frac{0}{73}\n\\right) = 0\\end{aligned}\n\n\n\\begin{aligned}\n-\\left(\\frac{73}{73}\\log_2\\frac{73}{73} + \\frac{0}{73}\\log_2\\frac{0}{73}\n\\right) = 0\\end{aligned}\n\n\n\n\nfor light branch the uncertainty will be following:\n\n\n\n\n\\begin{aligned}\n-\\left(\\frac{0}{27}\\log_2\\frac{0}{27} + \\frac{27}{27}\\log_2\\frac{27}{27}\n\\right) = 0\\end{aligned}\n\n\n\\begin{aligned}\n-\\left(\\frac{0}{27}\\log_2\\frac{0}{27} + \\frac{27}{27}\\log_2\\frac{27}{27}\n\\right) = 0\\end{aligned}\n\n\n\n\nthe condition entropy would be the weighted average over the branches:\n\n\n\n\n\\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{73}{100}*0 + \\frac{27}{100}*0 =  0\\end{aligned}\n\n\n\\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{73}{100}*0 + \\frac{27}{100}*0 =  0\\end{aligned}\n\n\n\n\nSo we choose the \nTraffic\nTraffic\n predictor as the root of decision tree\nbecause it will has the maximum information gain.\n\n\nRelationship between two decision trees\n\n\nT1:\n Given the data, the decision tree has been built on some\nparameter \n\\theta\n\\theta\n. \nT2:\n The data has been normalized by subtracting\nthe mean and divided by variance. The tree has been built on this new\nnormalized data.\n\n\n\n\nT1\n and \nT2\n will be same if the parameters used in building the\ndecision tree is function of data points else it will be different. Here\nis a simple example when \nT1\n and \nT2\n can be different suppose we\nhave five measurements of temperature as following\n\n\n\n\n\\{-90, -6, -1, 2, 5\\}\n\n\n\\{-90, -6, -1, 2, 5\\}\n\n\n\n\nNow we choose a parameter for splitting is \nT<0\nT<0\n belongs left subtree\nand \nT>0\nT>0\n to right. Lets call this \nT1\n. The mean of above data set\n\n-15\n-15\n, the variance is always going to be positive, so it is not going\nto make change, for this reason I am ignoring the variance. Let the\nvariance = \nk\nk\n. The new data set will look like following:\n\n\n\n\n\\{ -75/k, 9/k, 14/k, 17/k, 20/k\\}\n\n\n\\{ -75/k, 9/k, 14/k, 17/k, 20/k\\}\n\n\n\n\nNow if you use the same parameter to build the new tree we will have one\ndata point on left subtree and 4 data points on right subtree. Lets call\nthis \nT2\n. We can say for sure that \nT1\n and \nT2\n are different.\nSince we had the splitting parameter independent of the data points.\n\n\n\n\nIn case of parameters being dependent on the data points, the\naforementioned transformation will not change the characteristics of\ndata in new setting. If we expand the transformation of data point, it\nwill look like following:\n\n\n\n\n\\begin{aligned}\n X_i \\longrightarrow \\left(\\frac{X_i}{\\sigma^2} - \\frac{\\mu}{\\sigma^2}\\right)\n \\end{aligned}\n\n\n\\begin{aligned}\n X_i \\longrightarrow \\left(\\frac{X_i}{\\sigma^2} - \\frac{\\mu}{\\sigma^2}\\right)\n \\end{aligned}\n\n\n\n\nSo it is one to one mapping from old reference system to new one (\nscaled by \n\\frac{1}{\\sigma^2}\n\\frac{1}{\\sigma^2}\n and then subtracted by\n\n\\frac{\\mu}{\\sigma^2}\n\\frac{\\mu}{\\sigma^2}\n). So the order of data points remain the same in\nnew reference system and the same effect will reflect to the\ncorresponding parameter calculations. In conclusion the new tree \nT2\n\nwill be same as \nT1\n.\n\n\nComparison of Gini index and Cross entropy\n\n\nWe are given:\n\n\n\n\n\\begin{aligned}\nGI = \\sum_{k=1}^{K} p_k(1-p_k) \\end{aligned}\n\n\n\\begin{aligned}\nGI = \\sum_{k=1}^{K} p_k(1-p_k) \\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nCE = - \\sum_{k=1}^{K} p_k \\log p_k \\end{aligned}\n\n\n\\begin{aligned}\nCE = - \\sum_{k=1}^{K} p_k \\log p_k \\end{aligned}\n\n\n\n\n\n\n\\begin{aligned}\nGI - CE = \\sum_{k=1}^{K}p_k(1-p_k + \\log(p_k)) \\\\\n= \\sum_{k = 1}^{K} p_k(q_k + \\log(1-q_k))\\\\\n=\\sum_{k=1}^{K} p_k\\left(q_k + \\left( -q_k - \\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . . \n\\right)   \n\\right)\\\\\n= \\sum_{k=1}^{K} p_k\\left(-\\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . .\n\\right)<=0 \\\\\n\\hspace{20pt} \\\\\n\\text{becasue }p_k, & q_k \\text{ are both non-negative}\\end{aligned}\n\n\n\\begin{aligned}\nGI - CE = \\sum_{k=1}^{K}p_k(1-p_k + \\log(p_k)) \\\\\n= \\sum_{k = 1}^{K} p_k(q_k + \\log(1-q_k))\\\\\n=\\sum_{k=1}^{K} p_k\\left(q_k + \\left( -q_k - \\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . . \n\\right)   \n\\right)\\\\\n= \\sum_{k=1}^{K} p_k\\left(-\\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . .\n\\right)<=0 \\\\\n\\hspace{20pt} \\\\\n\\text{becasue }p_k, & q_k \\text{ are both non-negative}\\end{aligned}\n\n\n\n\nProgramming\n\n\n5.4 Performace comparision\n\n\nIn case of Naive bayes real valued feature has been used. I am changing\nthe code now, so I can not attach the new values, since it was informed\njust one day before the assignment submission. I will try my best to\nmodify the code and submit the electronic version compatible with binary\nfeature support. But I believe the accuracies will remain the same.\nPlease take this into consideration. The tables below contain the\nperformance of different methods.\n\n\n\n\n    K        Train accu.   Valid accu.   Test accu.\n\n\n\n[0.5ex] 1     77.7895       75.5784      79.4344\n        3          83.1579       80.4627      86.3753\n        5          86.6316       83.2905      90.7455\n        7          88.4211       84.0617      89.2031\n        9          88.6316       86.8895      89.4602\n       11          89.0526       86.3753      87.9177\n       13         188.4211       85.6041      87.9177\n       15          87.0526       82.7763      86.3753\n       17          85.8947       82.5193      86.1183\n       19          85.2632       82.2622      85.3470\n       21          85.3684       80.9769      84.8329\n       23          84.5263       82.5193      84.3188\n     [1ex]                                \n\n\n\n\n: Performance at kNN at different values of k\n\n\n\n\ntable:nonlin\n\n\ntable:nonlin\n\n\n\n\n\n\n MinLeaf     Train accu.   Valid accu.   Test accu.\n\n\n\n[0.5ex] 1     96.7368       93.3883      94.7636\n        2          96.7368       93.3883      94.7636\n        3          96.5263       93.9409      94.7636\n        4          96.5263       93.9409      94.7636\n        5          96.3158       94.1839      95.0642\n        6          96.2105       94.6859      94.8224\n        7          96.0000       94.9121      94.8797\n        8          95.8947       95.1871      95.3046\n        9          95.2632       94.6185      94.2199\n       10          94.7368       93.8334      94.2786\n     [1ex]                                \n\n\n\n\n: Decision Tree with different MinLeaf (Gini Index)\n\n\n\n\ntable:nonlin\n\n\ntable:nonlin\n\n\n\n\n\n\n MinLeaf     Train accu.   Valid accu.   Test accu.\n\n\n\n[0.5ex] 1     97.0526       93.3505      94.2163\n        2          97.0526       93.3505      94.2163\n        3          96.8421       93.9032      94.2163\n        4          96.8421       93.9032      94.2163\n        5          96.6316       94.1462      94.5168\n        6          96.5263       94.6482      94.2750\n        7          96.3158       94.8744      94.3323\n        8          95.8947       95.1871      95.3046\n        9          95.2632       94.6185      94.2199\n       10          94.7368       93.8334      94.2786\n     [1ex]                                \n\n\n\n\n: Decision Tree with different MinLeaf (Cross Entropy)\n\n\n\n\ntable:nonlin\n\n\ntable:nonlin\n\n\n\n\n\n\n      Method           Train accu.   Valid accu.   Test accu.\n\n\n\n[0.5ex] Naive Bayes      87.05         83.80        83.80\n       Logistic reg           83.89         81.49        85.35\n          [1ex]                                     \n\n\n\n\n: Performance of Naive Bayes and Logistic Reg.\n\n\n\n\ntable:nonlin\n\n\ntable:nonlin\n\n\n\n\n5.5 Decision Boundary\n\n\n\n\n\n\n\n\n\n\nAs we can see in the figure when the value of K increases, the decision\nboundary smoothen out. As we can see in case of K = 20, very less red\ndata points lying in the blue region and just one chunk of blue point\nlying on left bottom of the figure.",
            "title": "Homework 1"
        },
        {
            "location": "/solution_hw_1/#naive-bayes",
            "text": "",
            "title": "Naive Bayes"
        },
        {
            "location": "/solution_hw_1/#parametric-form-of-naive-bayes-with-gaussian-assumption",
            "text": "Feature vector  X \\in R^D X \\in R^D  and  Y Y  is a binary random vector.   Y \\sim Bern(\\pi)  Y \\sim Bern(\\pi)    X = \\{X_1, ..., X_D\\}  X = \\{X_1, ..., X_D\\}    P (X_j| Y = y_k ) \\sim N(\\mu_{jk}, \\sigma_j)  P (X_j| Y = y_k ) \\sim N(\\mu_{jk}, \\sigma_j)   Naive Bayes assumes the conditional independence of features given  Y Y   P(X_i = x_i, X_j = x_j | Y = y_k) = P (X_i = x_i | Y =y_k)*P(X_j = x_j| Y= y_k) \\space \\forall i \\neq j  P(X_i = x_i, X_j = x_j | Y = y_k) = P (X_i = x_i | Y =y_k)*P(X_j = x_j| Y= y_k) \\space \\forall i \\neq j   Here we have to show that the posterior probability can be written as\nthe posterior of logistic regression.Proceeding with above assumptions.   \\begin{aligned}\nP(Y = 1 | X ) = \\frac{P(X| Y = 1)*P(Y = 1)}{P(X)}\\end{aligned}  \\begin{aligned}\nP(Y = 1 | X ) = \\frac{P(X| Y = 1)*P(Y = 1)}{P(X)}\\end{aligned}    \\begin{aligned}\nP(X) = P(X|Y = 0)*P(Y = 0) + P (X| Y = 1)* P(Y = 1 )\\end{aligned}  \\begin{aligned}\nP(X) = P(X|Y = 0)*P(Y = 0) + P (X| Y = 1)* P(Y = 1 )\\end{aligned}   Using eq. (1.2) in (1.1) and bringing the numerator to denominator\nsimplifies eq. (1) to the following.   \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}{1 + \\frac{P(X| Y = 0)*P(Y = 0)}{P(X| Y = 1 )*P(Y = 1)}}\\end{aligned}  \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}{1 + \\frac{P(X| Y = 0)*P(Y = 0)}{P(X| Y = 1 )*P(Y = 1)}}\\end{aligned}    \\begin{aligned}\nP(X | Y = y_k) = \\prod_{i=1}^D P (X_i| Y = y_k)\\end{aligned}  \\begin{aligned}\nP(X | Y = y_k) = \\prod_{i=1}^D P (X_i| Y = y_k)\\end{aligned}    \\begin{aligned}\nP(X_i | Y = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_i}exp\\left(-\\frac{(X_i - \\mu_{ik})^2}{2\\sigma_{i}^2}\\right)\\end{aligned}  \\begin{aligned}\nP(X_i | Y = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_i}exp\\left(-\\frac{(X_i - \\mu_{ik})^2}{2\\sigma_{i}^2}\\right)\\end{aligned}    \\begin{aligned}\nP (Y = 1) = \\pi , P (Y = 0 ) = 1 -\\pi\\end{aligned}  \\begin{aligned}\nP (Y = 1) = \\pi , P (Y = 0 ) = 1 -\\pi\\end{aligned}   Using eq. (1.4),(1.5) and (1.6) in eq. (1.3) we get the simplified form\nas following   \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1 - \\pi}\n{\\pi} *\n\\frac{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}    \\right)     }\n{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}    \\right) } }\\end{aligned}  \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1 - \\pi}\n{\\pi} *\n\\frac{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}    \\right)     }\n{\\prod_{i = 1 }^{D} exp \\left(- \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}    \\right) } }\\end{aligned}   Solving further to simpler terms by assuming the following:   Z_{i0} = \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}  Z_{i0} = \\frac{(X_i - \\mu_{i0})^2}\n{2\\sigma_i^2}    Z_{i1} = \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}  Z_{i1} = \\frac{(X_i - \\mu_{i1})^2}\n{2\\sigma_i^2}    \\begin{aligned}\n Z_{i1} - Z_{i0}  = \\frac{(2X_i - (\\mu_{i0} + \\mu_{i1}) )*(\\mu_{i0} - \\mu_{i1})}{2\\sigma_i^2}\n \\end{aligned}  \\begin{aligned}\n Z_{i1} - Z_{i0}  = \\frac{(2X_i - (\\mu_{i0} + \\mu_{i1}) )*(\\mu_{i0} - \\mu_{i1})}{2\\sigma_i^2}\n \\end{aligned}    \\begin{aligned}\n= \\frac{X_i*(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}  - \\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}{2\\sigma_i^2}\\end{aligned}  \\begin{aligned}\n= \\frac{X_i*(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}  - \\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}{2\\sigma_i^2}\\end{aligned}   eq. (1.7) can be rewritten using simplified terms like  Z_{i1} - Z_{i0} Z_{i1} - Z_{i0} \nas following.   \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1-\\pi}\n{\\pi}*\\prod_{i=1}^{D}exp \\left( \\frac{-(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2} + \\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i   \\right)}\\end{aligned}  \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + \\frac{1-\\pi}\n{\\pi}*\\prod_{i=1}^{D}exp \\left( \\frac{-(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2} + \\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i   \\right)}\\end{aligned}   Product term in denominator of eq. (1.8) can be substituted with summand\ninside the exponential expression, and  \\frac{1-\\pi}{\\pi} \\frac{1-\\pi}{\\pi}  can be\nwritten as  exp\\left(-log\\frac{\\pi}{1 - \\pi}\\right). exp\\left(-log\\frac{\\pi}{1 - \\pi}\\right).  Doing above\nchanges to eq. (1.8) it reduces to the following form.   \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + exp \\left\\lbrace - \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i\n\\right\\rbrace}\\end{aligned}  \\begin{aligned}\nP(Y = 1 | X ) = \\frac{1}\n{1 + exp \\left\\lbrace - \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0} - \\mu_{i1})}{\\sigma_i^2}*X_i\n\\right\\rbrace}\\end{aligned}   We can see clearly in eq. (1.9) that the posterior probability is taking\nthe form of logistic regression. Comparing with the logistic regression\nexpression. We can write the parameters as follows:   \\begin{aligned}\n\\omega_0 = \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\\end{aligned}  \\begin{aligned}\n\\omega_0 = \\left(log\\frac{\\pi}{1 - \\pi}\n+ \\sum_{i = 1}^{D}\\frac{(\\mu_{i0}^2 - \\mu_{i1}^2)}\n{2\\sigma_i^2}\n\\right)\\end{aligned}    \\begin{aligned}\n\\textbf{w} = \\left[\\frac{(\\mu_{00} - \\mu_{01})}{\\sigma_1^2} , ... , \\frac{(\\mu_{D0} - \\mu_{D1})}{\\sigma_D^2}\\right]^T\\end{aligned}  \\begin{aligned}\n\\textbf{w} = \\left[\\frac{(\\mu_{00} - \\mu_{01})}{\\sigma_1^2} , ... , \\frac{(\\mu_{D0} - \\mu_{D1})}{\\sigma_D^2}\\right]^T\\end{aligned}",
            "title": "Parametric form of Naive Bayes with Gaussian Assumption"
        },
        {
            "location": "/solution_hw_1/#parametric-estimation-for-naive-bayes-with-gaussian-assumption",
            "text": "We have training set of size  N N  of the form (\\textbf{x}_\\textbf{i}, y_i) (\\textbf{x}_\\textbf{i}, y_i) , where   \\begin{aligned}\n\\textbf{x}_\\textbf{i} = (x_{i1}, ..., x_{iD})^T, y_i \\in \\{0,1\\}\\end{aligned}  \\begin{aligned}\n\\textbf{x}_\\textbf{i} = (x_{i1}, ..., x_{iD})^T, y_i \\in \\{0,1\\}\\end{aligned}   Since we have to estimate the distribution parameters such as \\pi_i, \\mu_{jk}, \\pi_i, \\mu_{jk},  and  \\sigma_j \\sigma_j . We can do this by writing log\nlikelihood and differentiating w.r.t corresponding parameters and set to\nzero to get the values. Following derivation are for the log likelihood\nand parameter estimation.  Joint probability can be written as   \\begin{aligned}\nP(X, Y ) = \\prod_{i = 1 }^{N} P (X_i, Y_i)= \\prod_{i = 1 }^{N} P(X_i | Y_i = y_k)*P (Y_i = y_k)\\end{aligned}  \\begin{aligned}\nP(X, Y ) = \\prod_{i = 1 }^{N} P (X_i, Y_i)= \\prod_{i = 1 }^{N} P(X_i | Y_i = y_k)*P (Y_i = y_k)\\end{aligned}    \\begin{aligned}\n= \\prod_{i = 1}^{N}P(Y_i = y_k)\\prod_{j = 1}^{D}P(X_{ij} = x_{ij} | Y_i = y_k)\\end{aligned}  \\begin{aligned}\n= \\prod_{i = 1}^{N}P(Y_i = y_k)\\prod_{j = 1}^{D}P(X_{ij} = x_{ij} | Y_i = y_k)\\end{aligned}   In above deduction we have used the naive bayes assumption. Now we know\nthe following:   \\begin{aligned}\nP(Y_i = y_k) = \\pi^{y_k}*(1-\\pi)^{(1-y_k)}\\end{aligned}  \\begin{aligned}\nP(Y_i = y_k) = \\pi^{y_k}*(1-\\pi)^{(1-y_k)}\\end{aligned}    \\begin{aligned}\nP(X_{ij} = x_{ij} | Y_i = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_j}exp\\left(-\\frac{(x_{ij} - \\mu_{jk})^2}{2\\sigma_{j}^2}\\right)\\end{aligned}  \\begin{aligned}\nP(X_{ij} = x_{ij} | Y_i = y_k) = \\frac{1}{(2\\pi)^{1/2}\\sigma_j}exp\\left(-\\frac{(x_{ij} - \\mu_{jk})^2}{2\\sigma_{j}^2}\\right)\\end{aligned}   Since we know that log likelihood is the log of joint pdf. We can take\nthe log on both side of eq. (1.10) to get the log likelihood.   \\begin{aligned}\nl(\\theta | X, Y) = logP(X,Y) = \\sum_{i = 1}^{N}\\left( log P(Y_i = y_k) + \\sum_{j = 1}^{D} log P (X_{ij} = x_{ij} | Y_i = y_k)\n\\right)  \\end{aligned}  \\begin{aligned}\nl(\\theta | X, Y) = logP(X,Y) = \\sum_{i = 1}^{N}\\left( log P(Y_i = y_k) + \\sum_{j = 1}^{D} log P (X_{ij} = x_{ij} | Y_i = y_k)\n\\right)  \\end{aligned}    \\begin{aligned}\n= \\sum_{i = 1}^{N} \\left( y_klog\\pi + (1-y_k)log(1 - \\pi) + \\sum_{j = 1}^{D} \\left( -log ( (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{jk})^2}\n{2\\sigma_j^2}\n\\right)\n\\right)\\end{aligned}  \\begin{aligned}\n= \\sum_{i = 1}^{N} \\left( y_klog\\pi + (1-y_k)log(1 - \\pi) + \\sum_{j = 1}^{D} \\left( -log ( (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{jk})^2}\n{2\\sigma_j^2}\n\\right)\n\\right)\\end{aligned}   Suppose there are m data points with label 1 and N-m with label 0, above\nequation can be rewritten as following:   \\begin{aligned}\nl(\\theta | X, Y) = \\sum_{i:Y_i = y_k = 1}y_klog\\pi  + \\sum_{i:Y_i = y_k = 0}(1-y_k)log(1-\\pi) \\\\ + \\sum_{i:Y_i = y_k = 1}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j1})^2}{2\\sigma} \n\\right) \\\\ +  \\sum_{i:Y_i = y_k = 0}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j0})^2}{2\\sigma} \n\\right)\\end{aligned}  \\begin{aligned}\nl(\\theta | X, Y) = \\sum_{i:Y_i = y_k = 1}y_klog\\pi  + \\sum_{i:Y_i = y_k = 0}(1-y_k)log(1-\\pi) \\\\ + \\sum_{i:Y_i = y_k = 1}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j1})^2}{2\\sigma} \n\\right) \\\\ +  \\sum_{i:Y_i = y_k = 0}\\sum_{j=1}^{D}\\left( -log (2\\pi)^{1/2}\\sigma_j - \\frac{(x_{ij} - \\mu_{j0})^2}{2\\sigma} \n\\right)\\end{aligned}   Calculating MLE for  \\pi \\pi :   \\begin{aligned}\n\\hat{\\pi} = \\left\\lbrace \\pi: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = 0  \\right\\rbrace\\end{aligned}  \\begin{aligned}\n\\hat{\\pi} = \\left\\lbrace \\pi: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = 0  \\right\\rbrace\\end{aligned}    \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = \\frac{m}{\\pi} - \\frac{(N - m)}{1 - \\pi} + 0 + 0  \\end{aligned}  \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi} = \\frac{m}{\\pi} - \\frac{(N - m)}{1 - \\pi} + 0 + 0  \\end{aligned}   Equating the above equation to zero and solving for  \\pi \\pi   \\begin{aligned}\nm(1 - \\pi)  - (N-m)\\pi = 0 \n\\implies m - N\\pi = 0 \n\\implies \\pi = \\frac{m}{N} \\end{aligned}  \\begin{aligned}\nm(1 - \\pi)  - (N-m)\\pi = 0 \n\\implies m - N\\pi = 0 \n\\implies \\pi = \\frac{m}{N} \\end{aligned}   So the likelihood parameter estimation of  \\pi \\pi  is:   \\begin{aligned}\n\\hat{\\pi} = \\frac{m}{N} = \\frac{\\#label = 1 }{N}\\end{aligned}  \\begin{aligned}\n\\hat{\\pi} = \\frac{m}{N} = \\frac{\\#label = 1 }{N}\\end{aligned}   Likelihood for  \\mu_{jk} \\mu_{jk} :   \\begin{aligned}\n\\hat{\\mu}_{jk} = \\left\\lbrace \\mu_{jk}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\mu_{jk}} = 0  \\right\\rbrace\\end{aligned}  \\begin{aligned}\n\\hat{\\mu}_{jk} = \\left\\lbrace \\mu_{jk}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\mu_{jk}} = 0  \\right\\rbrace\\end{aligned}   We can do this in two parts, k = 0 and k = 1 for k = 0, we shall only\nhave the fourth term in partial differentiation, other term will give\nzeros.   \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi}  = \\sum_{i:Y_i = y_k = 0}- \\frac{(x_{ij} - \\mu_{jo})}{\\sigma_j^2}*(-1) = 0 \\\\\n\\implies \\sum_{i:Y_i = y_k = 0}(x_{ij} - \\mu_{jo}) = 0 \\end{aligned}  \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\pi}  = \\sum_{i:Y_i = y_k = 0}- \\frac{(x_{ij} - \\mu_{jo})}{\\sigma_j^2}*(-1) = 0 \\\\\n\\implies \\sum_{i:Y_i = y_k = 0}(x_{ij} - \\mu_{jo}) = 0 \\end{aligned}   From above assumption we know the count of zero labels are  N-m N-m  Solving\nabove equality gives us the following   \\begin{aligned}\n(N-m)*\\mu_{j0} = \\sum_{i:Y_i = y_k = 0} x_{ij}\\end{aligned}  \\begin{aligned}\n(N-m)*\\mu_{j0} = \\sum_{i:Y_i = y_k = 0} x_{ij}\\end{aligned}   which gives us   \\begin{aligned}\n\\hat{\\mu}_{j0} = \\frac{\\sum_{i:Y_i = y_k = 0} x_{ij}}{(N-m)}\\end{aligned}  \\begin{aligned}\n\\hat{\\mu}_{j0} = \\frac{\\sum_{i:Y_i = y_k = 0} x_{ij}}{(N-m)}\\end{aligned}   Similarly we get the estimate for  \\mu_{j1} \\mu_{j1}   \\begin{aligned}\n\\hat{\\mu}_{j1} = \\frac{\\sum_{i:Y_i = y_k = 1} x_{ij}}{(m)}\\end{aligned}  \\begin{aligned}\n\\hat{\\mu}_{j1} = \\frac{\\sum_{i:Y_i = y_k = 1} x_{ij}}{(m)}\\end{aligned}   Likelihood for  \\sigma_{j} \\sigma_{j} :   \\begin{aligned}\n\\hat{\\sigma}_{j} = \\left\\lbrace \\sigma_{j}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}} = 0  \\right\\rbrace\\end{aligned}  \\begin{aligned}\n\\hat{\\sigma}_{j} = \\left\\lbrace \\sigma_{j}: \\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}} = 0  \\right\\rbrace\\end{aligned}    \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}}  = \\sum_{i:Y_i =y_k = 1} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j1})^2}{\\sigma_{j}^3}\n\\right)\\\\\n+ \\sum_{i:Y_i = y_k = 0} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j0})^2}{\\sigma_{j}^3}\n\\right)\\\\\n= - \\frac{N}{\\sigma_{j}} + \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{\\sigma_{j}^3}\\end{aligned}  \\begin{aligned}\n\\frac{\\partial l(\\theta|X,Y)}{\\partial\\sigma_{j}}  = \\sum_{i:Y_i =y_k = 1} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j1})^2}{\\sigma_{j}^3}\n\\right)\\\\\n+ \\sum_{i:Y_i = y_k = 0} \\left( -\\frac{1}{\\sigma_{j}} + \\frac{(x_{ij} - \\mu_{j0})^2}{\\sigma_{j}^3}\n\\right)\\\\\n= - \\frac{N}{\\sigma_{j}} + \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{\\sigma_{j}^3}\\end{aligned}   Equating it to zero will give the following:   \\begin{aligned}\n\\sigma_{j}^2 = \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}\\\\\n\\implies \\hat{\\sigma}_{j} = \\sqrt{\\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}}\\end{aligned}  \\begin{aligned}\n\\sigma_{j}^2 = \\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}\\\\\n\\implies \\hat{\\sigma}_{j} = \\sqrt{\\frac{\\sum_{i = 1 }^{N}\\left( y_k (x_{ij} - \\mu_{j1})^2 + (1 - y_k ) (x_{ij} - \\mu_{j0})^2\\right)}\n{N}}\\end{aligned}",
            "title": "Parametric estimation for Naive Bayes with Gaussian assumption"
        },
        {
            "location": "/solution_hw_1/#nearest-neighbor",
            "text": "",
            "title": "Nearest Neighbor"
        },
        {
            "location": "/solution_hw_1/#feature-weighting-in-low-dimension",
            "text": "Feature weighting is required in case of noise, because noise affect the\neuclidean distance measures which can lead to poor performance of KNN.\nThis is also known as Achilles\u2019 heel of KNN. In case of low dimension D = 3 D = 3  with discretized values of weights in each dimension. We can do\ngrid search to look for the optimal weights which can give the best\ntraining accuracy. The procedure explained below can be used to find the\noptimal weights for the features. Here we have following assumption   \\begin{aligned}\nX_i = \\left(x_{i1}, x_{i2}, x_{i3}\\right)^T\\\\\nW = \\left(w_1, w_2, w_3\\right)\\\\\nw_i \\in \\{w_{i1}, w_{i2}, .,.,., w_{ik}\\}\\end{aligned}  \\begin{aligned}\nX_i = \\left(x_{i1}, x_{i2}, x_{i3}\\right)^T\\\\\nW = \\left(w_1, w_2, w_3\\right)\\\\\nw_i \\in \\{w_{i1}, w_{i2}, .,.,., w_{ik}\\}\\end{aligned}   The weights has been discretized to  k k  values. We can just do grid\nsearch by using following pseudo code. opt (W) = {} for w_1 = w_{11}:w_{1k} w_1 = w_{11}:w_{1k}  for  w_2 = w_{21}:w_{2k} w_2 = w_{21}:w_{2k}  for  w_3 = w_{31}:w_{3k} w_3 = w_{31}:w_{3k}  for  i = 1:N i = 1:N  for  j\\neq i = 1:N j\\neq i = 1:N  compute  d(X_i, X_j) d(X_i, X_j)  end  end  Compute the training accuracy and update opt(W) if accuracy is  better.  end  end  end  For this approach we can also use dynamic programming to reduce the\ncomputation, we can maintain the  N N x N N x D D  matrix with entries of\neuclidean distances of feature vectors. Afterwards we can use this\nmatrix as lookup table and just multiply with the weights to get the\nweighted feature distance. Using the Dynamic programming approach will\nlead to following complexity: Time complexity:   O(k^3)  O(k^3)   Space complexity:   O(N^2)  O(N^2)",
            "title": "Feature weighting in low dimension"
        },
        {
            "location": "/solution_hw_1/#feature-weighting-in-higher-dimensions",
            "text": "In higher dimension above approach of finding optimal weight will lead\nto exponential time complexity  O(k^D) O(k^D) . This is curse of\ndimensionality. As being instructed by TA that we do not have to use any\nstatistical approach to construct the method for assigning weights to\nfeatures (We have to design algorithm). If we present the weighted\ndistances calculation in matrix form, it will appear as follows:   \\begin{aligned}\n\\hat{Label}(X_1) = Label \\left(X_j : j = \\min_{j}\n\\begin{bmatrix}\nw_1 &  . &  w_D\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_{11} - x_{21})^2 &  (x_{11} - x_{31})^2 &  . & (x_{11} - x_{N1})^2 \\\\\n(x_{12} - x_{22})^2  & (x_{12} - x_{32})^2 &  . & (x_{12} - x_{N2})^2 \\\\\n.  & . & .  & . & \\\\\n.  & . & .  & . & \\\\\n(x_{1D} - x_{2D})^2 & (x_{1D} - x_{3D})^2  &  . & (x_{1D} - x_{ND})^2\n\\end{bmatrix}\n\\right)\\end{aligned}  \\begin{aligned}\n\\hat{Label}(X_1) = Label \\left(X_j : j = \\min_{j}\n\\begin{bmatrix}\nw_1 &  . &  w_D\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_{11} - x_{21})^2 &  (x_{11} - x_{31})^2 &  . & (x_{11} - x_{N1})^2 \\\\\n(x_{12} - x_{22})^2  & (x_{12} - x_{32})^2 &  . & (x_{12} - x_{N2})^2 \\\\\n.  & . & .  & . & \\\\\n.  & . & .  & . & \\\\\n(x_{1D} - x_{2D})^2 & (x_{1D} - x_{3D})^2  &  . & (x_{1D} - x_{ND})^2\n\\end{bmatrix}\n\\right)\\end{aligned}   We do above calculation for every data point. Since there are  k^D k^D \npossible matrices  [w_1 ... w_D ] [w_1 ... w_D ] , we can not calculate this in\npolynomial time. One way to select meaningful features from the haystack\nis to calculate the training accuracy on each individual feature by\nsetting other\u2019s weight to zero. we assign the weight to be training\naccuracy of that individual feature. We can decide how many feature we\nhave to take into consideration. This will result into the limited\nfeatures with weight assigned on each of them.  2 nd  method  Objective\nfunction for above problem will look like following:   \\begin{aligned}\nJ(W) = \\sum_{i=1}^{N}\\left( \\hat{Label}(X_i) - Label (X_i) \\right)\\end{aligned}  \\begin{aligned}\nJ(W) = \\sum_{i=1}^{N}\\left( \\hat{Label}(X_i) - Label (X_i) \\right)\\end{aligned}   The characteristic of above objective function is a curve in higher\ndimension with multiple local minimum. We can use the gradient descent\napproach to find the local minima\u2019s we can start with different set of\nrandom weight vectors and try to find the different minimum, select the\nbest one out of these and return those weight vectors as weighted\nfeature vector. This is a polynomial time algorithm, but of course this\nmay not give the optimal solution.",
            "title": "Feature weighting in higher dimensions"
        },
        {
            "location": "/solution_hw_1/#logistic-regression",
            "text": "",
            "title": "Logistic regression"
        },
        {
            "location": "/solution_hw_1/#negative-log-likelihood-or-loss-function",
            "text": "\\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right)\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right)\\end{aligned}   We know that   \\begin{aligned}\nP (Y = 1 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})} = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}  \\begin{aligned}\nP (Y = 1 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})} = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}    \\begin{aligned}\nP (Y = 0 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) =  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}  \\begin{aligned}\nP (Y = 0 | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) =  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})\\\\\\end{aligned}    \\begin{aligned}\nP (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})^{y_i} (  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )^{1-y_i}\\\\\n\\log P (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} )  = y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )\\end{aligned}  \\begin{aligned}\nP (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} ) = \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})^{y_i} (  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )^{1-y_i}\\\\\n\\log P (Y = y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}} )  = y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) )\\end{aligned}   Writing the negative log likelihood in simpler form using above\nexpression   \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\log(P (Y = y_i| \\textbf{X} = \\textbf{x}_{\\textbf{i}}))\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\log(P (Y = y_i| \\textbf{X} = \\textbf{x}_{\\textbf{i}}))\\end{aligned}    \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\left(y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) ) \n\\right)\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = - \\sum_{i = 1 }^{n} \\left(y_i \\log \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) + (1-y_i)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}}) ) \n\\right)\\end{aligned}",
            "title": "Negative log likelihood or Loss function"
        },
        {
            "location": "/solution_hw_1/#proof-for-convexity-of-loss-function",
            "text": "To prove the convexity we are going to show that the hessian matrix will\nbe positive semidefinite. In order to make the derivation simpler we are\ngoing to use the property of convex functions. If  f(x) f(x)  and  g(x) g(x)  are\ntwo convex function then their sum  h(x) h(x)  is also going to be the convex\nfunction Proof: As we know from the property of convex function   f(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2)  f(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2)    g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda g(x_1) + (1 - \\lambda)g(x_2)  g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda g(x_1) + (1 - \\lambda)g(x_2)   Adding above two equations we get the following:   f(\\lambda x_1 + (1-\\lambda)x_2 ) + g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2) + \\lambda g(x_1) + (1 - \\lambda)g(x_2)  f(\\lambda x_1 + (1-\\lambda)x_2 ) + g(\\lambda x_1 + (1-\\lambda)x_2 ) <= \\lambda f(x_1) + (1 - \\lambda)f(x_2) + \\lambda g(x_1) + (1 - \\lambda)g(x_2)    h(\\lambda x_1 + (1-\\lambda)x_2 )) <= \\lambda h(x_1) + (1 - \\lambda)h(x_2)  h(\\lambda x_1 + (1-\\lambda)x_2 )) <= \\lambda h(x_1) + (1 - \\lambda)h(x_2)   So  h(x) h(x)  is also convex. Multiplying by any positive constant preserves\nthe convexity. Considering just one term from loss function.   \\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\end{aligned}  \\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\end{aligned}    \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i)\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i)\\end{aligned}   If we prove that $T(\\textbf{w,x},y) $ is convex then we can say that \\mathcal{L}(\\textbf{w}) \\mathcal{L}(\\textbf{w})  is convex (by above proved lemma).   \\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\\\\n= -y\\log \\left(\\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) - (1-y)\\log\\left(1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n=y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y)\\log\\left(\\frac{\\exp(-\\textbf{w}^{T}\\textbf{x})}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n= y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y) \\left( -\\textbf{w}^{T}\\textbf{x} - \\log\\left(  1 + \\exp(-\\textbf{w}^{T}\\textbf{x}\n\\right)\n\\right)\\\\\n=(1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\\end{aligned}  \\begin{aligned}\nT(\\textbf{w,x},y) = - \\left(y \\log \\sigma(\\textbf{w}^{T}\\textbf{x}) + (1-y)\\log(  1 - \\sigma(\\textbf{w}^{T}\\textbf{x}) ) \n\\right)\\\\\n= -y\\log \\left(\\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) - (1-y)\\log\\left(1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n=y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y)\\log\\left(\\frac{\\exp(-\\textbf{w}^{T}\\textbf{x})}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right)\\\\\n= y\\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right) - (1-y) \\left( -\\textbf{w}^{T}\\textbf{x} - \\log\\left(  1 + \\exp(-\\textbf{w}^{T}\\textbf{x}\n\\right)\n\\right)\\\\\n=(1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)\\end{aligned}   Finally we get:   T(\\textbf{w,x},y) = (1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)  T(\\textbf{w,x},y) = (1-y) \\textbf{w}^{T}\\textbf{x} + \\log \\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)   We need to find the hessian (H) (H)  of this which is basically  D D x D D \nmatrix and the element  H(j,k) H(j,k)  is:   \\begin{aligned}\nH(j,k) = \\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k}\\end{aligned}  \\begin{aligned}\nH(j,k) = \\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k}\\end{aligned}   Now calculating the  H(j,k) H(j,k) . Calculating the first derivative.   \\begin{aligned}\n\\frac{\\partial T(\\textbf{w,x},y)}{\\partial w_j} = \\frac{-x_j * \\exp(-\\textbf{w}^{T}\\textbf{x})}{ 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})} + x_j(1-y)\\\\\n= -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) +x_j(1-y)\\end{aligned}  \\begin{aligned}\n\\frac{\\partial T(\\textbf{w,x},y)}{\\partial w_j} = \\frac{-x_j * \\exp(-\\textbf{w}^{T}\\textbf{x})}{ 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})} + x_j(1-y)\\\\\n= -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x})}\n\\right) +x_j(1-y)\\end{aligned}   Differentiating above w.r.t.  w_k w_k   \\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = 0 + x_j \\left[ \\frac{(-1)*\\exp(-\\textbf{w}^{T}\\textbf{x})*(-x_k) }{\\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)^2}\n\\right] + 0 \\\\\n= x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k\\end{aligned}  \\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = 0 + x_j \\left[ \\frac{(-1)*\\exp(-\\textbf{w}^{T}\\textbf{x})*(-x_k) }{\\left( 1 + \\exp(-\\textbf{w}^{T}\\textbf{x})\n\\right)^2}\n\\right] + 0 \\\\\n= x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k\\end{aligned}    \\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k \\end{aligned}  \\begin{aligned}\n\\frac{\\partial^2 T(\\textbf{w,x},y)}{\\partial w_j \\partial w_k} = x_j * \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x}))*x_k \\end{aligned}   Note:  \\sigma(x) >= 0 \\hspace{8pt} \\forall x \\sigma(x) >= 0 \\hspace{8pt} \\forall x  If we fill out the hessian matrix it will look like following:   \\begin{aligned}\nH = \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x})) \\begin{bmatrix}\nx_1^2 & x_1x_2 &  . & .& . & x_1x_D\\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\nx_Dx_1 & x_Dx_2 & . & . & . & x_D^2\n\\end{bmatrix}\\end{aligned}  \\begin{aligned}\nH = \\sigma(\\textbf{w}^{T}\\textbf{x})*(1-\\sigma(\\textbf{w}^{T}\\textbf{x})) \\begin{bmatrix}\nx_1^2 & x_1x_2 &  . & .& . & x_1x_D\\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\n.  & . & . & . & . & . & \\\\\nx_Dx_1 & x_Dx_2 & . & . & . & x_D^2\n\\end{bmatrix}\\end{aligned}   Let\n$ K =  \\sigma(\\textbf{w} {T}\\textbf{x})*(1-\\sigma(\\textbf{w} {T}\\textbf{x}))$\nwhich is always greater than or equal to zero, matrix in above\nexpression can be decomposed into  XX^T XX^T  where X is a column matrix. So H H  can be rewritten as   \\begin{aligned}\nH = K XX^T\\end{aligned}  \\begin{aligned}\nH = K XX^T\\end{aligned}   where   X = \\begin{bmatrix}\nx_1 & x_2 & . & . & .& . x_D\n\\end{bmatrix}^T  X = \\begin{bmatrix}\nx_1 & x_2 & . & . & .& . x_D\n\\end{bmatrix}^T   H H  is a positive semidefinite matrix because if we take any vector  V V \nand calculate  V^THV V^THV  then it is always $>= 0 $. Following is the proof.   \\begin{aligned}\nV^THV = V^T KXX^T V = K (X^TV)^T (X^TV) = K ||X^TV||^2 >=0 \\hspace{20pt} \\end{aligned}  \\begin{aligned}\nV^THV = V^T KXX^T V = K (X^TV)^T (X^TV) = K ||X^TV||^2 >=0 \\hspace{20pt} \\end{aligned}   Since the hessian is PSD we can say that function is convex. Referring\nto eq.(3.2) the loss function is just the linear combination of  T(.) T(.) .\nAs we proved in the lemma that sum of convex functions is a convex\nfunction. Hence it is proved that the loss function is convex.",
            "title": "Proof for convexity of loss function"
        },
        {
            "location": "/solution_hw_1/#magnitude-of-optimal-w",
            "text": "Considering the binary logistic regression and samples are linearly\nseparable.",
            "title": "Magnitude of optimal w"
        },
        {
            "location": "/solution_hw_1/#regularized-logistic-regression",
            "text": "Since optimal  \\textbf{w} \\longrightarrow \\infty \\textbf{w} \\longrightarrow \\infty , in order to handle\nthe numerical instability the regularization term is being added to the\nloss function, and the regularized linear regression looks like\nfollowing:   \\begin{aligned}\n\\mathcal{L} (\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right) + \\lambda||\\textbf{w}||_2^2\\end{aligned}  \\begin{aligned}\n\\mathcal{L} (\\textbf{w}) = -\\log\\left( \\prod_{1= 1}^{n}P (Y = Y_i | \\textbf{X} = \\textbf{x}_{\\textbf{i}})\n\\right) + \\lambda||\\textbf{w}||_2^2\\end{aligned}   Computing gradient w.r.t.  \\omega_i \\omega_i . Referring to eq. (3.2) and the\nfirst derivative of  T(\\textbf{w,x},y) T(\\textbf{w,x},y)   \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) +  \\lambda||\\textbf{w}||_2^2\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}) = \\sum_{i = 1}^{n}T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) +  \\lambda||\\textbf{w}||_2^2\\end{aligned}    \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =  \\sum_{i = 1 }^{n} \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} + 2\\lambda \\omega_j\\end{aligned}  \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =  \\sum_{i = 1 }^{n} \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} + 2\\lambda \\omega_j\\end{aligned}   We have already calculated the term \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} \\frac{\\partial T(\\textbf{w,}\\textbf{x}_{\\textbf{i}},y_i) }{\\partial \\omega_j} \nin the process of proving the hessian matrix to be PSD. We can just\nplugin that term here. After plugging in the term we get the following :   \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right) +x_j(1-y_i)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}  \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( 1 - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right) +x_j(1-y_i)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}   Further simplifying above equation(  x_j x_j  cancels out ).   \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( y_i - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right)\n\\right)\n+ 2\\lambda\\omega_j \\\\\n= -\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}  \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  = \\sum_{i = 1 }^{n} \\left( -x_j\\left( y_i - \\frac{1}{1 + \\exp(-\\textbf{w}^{T}\\textbf{x}_{\\textbf{i}})}\n\\right)\n\\right)\n+ 2\\lambda\\omega_j \\\\\n= -\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}    \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =\n-\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}  \\begin{aligned}\n\\frac{\\partial \\mathcal{L}(\\textbf{w})}{\\partial \\omega_j}  =\n-\\sum_{i = 1 }^{n} \\left( x_j\\left( y_i - P(Y_i = 1 |X = \\textbf{x}_{\\textbf{i}} )\n\\right)\n\\right)\n+ 2\\lambda\\omega_j\\end{aligned}",
            "title": "Regularized logistic regression"
        },
        {
            "location": "/solution_hw_1/#unique-solution-of-the-regularized-loss-function",
            "text": "Since we have already proven that the loss function without\nregularization is convex and the regularization term is also convex\n(because  \\lambda > 0 \\lambda > 0  and  ||\\textbf{w}||^2 > 0 ||\\textbf{w}||^2 > 0 ) so the linear\ncombination will always be convex. There will be  \\textbf{w}^* \\textbf{w}^*  for\nwhich   \\begin{aligned}\n\\mathcal{L}(\\textbf{w}^*) <\\mathcal{L}(\\textbf{w}) \\hspace{10pt} \\forall \\textbf{w}\\neq \\textbf{w}^*\\in \\textbf{W}\\end{aligned}  \\begin{aligned}\n\\mathcal{L}(\\textbf{w}^*) <\\mathcal{L}(\\textbf{w}) \\hspace{10pt} \\forall \\textbf{w}\\neq \\textbf{w}^*\\in \\textbf{W}\\end{aligned}   We can start at any  \\textbf{w}_0 \\textbf{w}_0  and reach  \\textbf{w}^* \\textbf{w}^*  by using\ngradient descent approach.",
            "title": "Unique solution of the regularized loss function"
        },
        {
            "location": "/solution_hw_1/#decision-tree",
            "text": "",
            "title": "Decision Tree"
        },
        {
            "location": "/solution_hw_1/#building-a-decision-tree",
            "text": "Selection of predictor to form the decision tree  We shall be\nchoosing the predictor which can maximize the information gain.   \\begin{aligned}\nIG(Predictor) = H(Target) - H(Target|Predictor)\\\\\\end{aligned}  \\begin{aligned}\nIG(Predictor) = H(Target) - H(Target|Predictor)\\\\\\end{aligned}   Where  H(Target) H(Target)  is the entropy of target and  H(Target|Predictor) H(Target|Predictor)  is\nconditional entropy over predictor which is basically weighted sum of\nentropies of each branch after splitting using the predictor Here we\nhave   \\begin{aligned}\nPredictor \\in \\{Weather, Traffic\\}\n,\\hspace{10pt}Taget \\in \\{Accident\\hspace{3pt}rate\\}\\end{aligned}  \\begin{aligned}\nPredictor \\in \\{Weather, Traffic\\}\n,\\hspace{10pt}Taget \\in \\{Accident\\hspace{3pt}rate\\}\\end{aligned}    \\begin{aligned}\nIG(Weather) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Weather)\\\\\nIG(Traffic) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Traffic)\\end{aligned}  \\begin{aligned}\nIG(Weather) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Weather)\\\\\nIG(Traffic) = H(Accident\\hspace{3pt}rate ) - H(Accident\\hspace{3pt}rate|Traffic)\\end{aligned}   Looking at eq. (4.1) and (4.2) we can say that the predictor which gives\nminimum conditional entropy will lead to maximum information gain. So we\njust calculate the conditional entropy and decide the predictor to do\nthe splitting in order to form decision tree.  Case 1:  Choosing  Weather Weather  as root for sunny branch the uncertainty\nwill be following:   \\begin{aligned}\n-\\left(\\frac{23}{28}\\log_2\\frac{23}{28} + \\frac{5}{28}\\log_2\\frac{5}{28}\n\\right) = 0.6769\\end{aligned}  \\begin{aligned}\n-\\left(\\frac{23}{28}\\log_2\\frac{23}{28} + \\frac{5}{28}\\log_2\\frac{5}{28}\n\\right) = 0.6769\\end{aligned}   for rainy branch the uncertainty will be following:   \\begin{aligned}\n-\\left(\\frac{50}{72}\\log_2\\frac{50}{72} + \\frac{22}{72}\\log_2\\frac{22}{72}\n\\right) = 0.8880\\end{aligned}  \\begin{aligned}\n-\\left(\\frac{50}{72}\\log_2\\frac{50}{72} + \\frac{22}{72}\\log_2\\frac{22}{72}\n\\right) = 0.8880\\end{aligned}   the condition entropy would be the weighted average over the branches:   \\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{28}{100}*0.6769 + \\frac{72}{100}*0.8880 =  0.8289\\end{aligned}  \\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{28}{100}*0.6769 + \\frac{72}{100}*0.8880 =  0.8289\\end{aligned}   Case 2:  Choosing  Traffic Traffic  as root for heavy branch the uncertainty\nwill be following:   \\begin{aligned}\n-\\left(\\frac{73}{73}\\log_2\\frac{73}{73} + \\frac{0}{73}\\log_2\\frac{0}{73}\n\\right) = 0\\end{aligned}  \\begin{aligned}\n-\\left(\\frac{73}{73}\\log_2\\frac{73}{73} + \\frac{0}{73}\\log_2\\frac{0}{73}\n\\right) = 0\\end{aligned}   for light branch the uncertainty will be following:   \\begin{aligned}\n-\\left(\\frac{0}{27}\\log_2\\frac{0}{27} + \\frac{27}{27}\\log_2\\frac{27}{27}\n\\right) = 0\\end{aligned}  \\begin{aligned}\n-\\left(\\frac{0}{27}\\log_2\\frac{0}{27} + \\frac{27}{27}\\log_2\\frac{27}{27}\n\\right) = 0\\end{aligned}   the condition entropy would be the weighted average over the branches:   \\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{73}{100}*0 + \\frac{27}{100}*0 =  0\\end{aligned}  \\begin{aligned}\nH(Accident\\hspace{3pt}rate | Weather) = \\frac{73}{100}*0 + \\frac{27}{100}*0 =  0\\end{aligned}   So we choose the  Traffic Traffic  predictor as the root of decision tree\nbecause it will has the maximum information gain.",
            "title": "Building a decision tree"
        },
        {
            "location": "/solution_hw_1/#relationship-between-two-decision-trees",
            "text": "T1:  Given the data, the decision tree has been built on some\nparameter  \\theta \\theta .  T2:  The data has been normalized by subtracting\nthe mean and divided by variance. The tree has been built on this new\nnormalized data.",
            "title": "Relationship between two decision trees"
        },
        {
            "location": "/solution_hw_1/#comparison-of-gini-index-and-cross-entropy",
            "text": "We are given:   \\begin{aligned}\nGI = \\sum_{k=1}^{K} p_k(1-p_k) \\end{aligned}  \\begin{aligned}\nGI = \\sum_{k=1}^{K} p_k(1-p_k) \\end{aligned}    \\begin{aligned}\nCE = - \\sum_{k=1}^{K} p_k \\log p_k \\end{aligned}  \\begin{aligned}\nCE = - \\sum_{k=1}^{K} p_k \\log p_k \\end{aligned}    \\begin{aligned}\nGI - CE = \\sum_{k=1}^{K}p_k(1-p_k + \\log(p_k)) \\\\\n= \\sum_{k = 1}^{K} p_k(q_k + \\log(1-q_k))\\\\\n=\\sum_{k=1}^{K} p_k\\left(q_k + \\left( -q_k - \\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . . \n\\right)   \n\\right)\\\\\n= \\sum_{k=1}^{K} p_k\\left(-\\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . .\n\\right)<=0 \\\\\n\\hspace{20pt} \\\\\n\\text{becasue }p_k, & q_k \\text{ are both non-negative}\\end{aligned}  \\begin{aligned}\nGI - CE = \\sum_{k=1}^{K}p_k(1-p_k + \\log(p_k)) \\\\\n= \\sum_{k = 1}^{K} p_k(q_k + \\log(1-q_k))\\\\\n=\\sum_{k=1}^{K} p_k\\left(q_k + \\left( -q_k - \\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . . \n\\right)   \n\\right)\\\\\n= \\sum_{k=1}^{K} p_k\\left(-\\frac{q_k^2}{2} - \\frac{q_k^3}{3} . . .\n\\right)<=0 \\\\\n\\hspace{20pt} \\\\\n\\text{becasue }p_k, & q_k \\text{ are both non-negative}\\end{aligned}",
            "title": "Comparison of Gini index and Cross entropy"
        },
        {
            "location": "/solution_hw_1/#programming",
            "text": "",
            "title": "Programming"
        },
        {
            "location": "/solution_hw_1/#54-performace-comparision",
            "text": "In case of Naive bayes real valued feature has been used. I am changing\nthe code now, so I can not attach the new values, since it was informed\njust one day before the assignment submission. I will try my best to\nmodify the code and submit the electronic version compatible with binary\nfeature support. But I believe the accuracies will remain the same.\nPlease take this into consideration. The tables below contain the\nperformance of different methods.       K        Train accu.   Valid accu.   Test accu.  [0.5ex] 1     77.7895       75.5784      79.4344\n        3          83.1579       80.4627      86.3753\n        5          86.6316       83.2905      90.7455\n        7          88.4211       84.0617      89.2031\n        9          88.6316       86.8895      89.4602\n       11          89.0526       86.3753      87.9177\n       13         188.4211       85.6041      87.9177\n       15          87.0526       82.7763      86.3753\n       17          85.8947       82.5193      86.1183\n       19          85.2632       82.2622      85.3470\n       21          85.3684       80.9769      84.8329\n       23          84.5263       82.5193      84.3188\n     [1ex]                                   : Performance at kNN at different values of k   table:nonlin  table:nonlin     MinLeaf     Train accu.   Valid accu.   Test accu.  [0.5ex] 1     96.7368       93.3883      94.7636\n        2          96.7368       93.3883      94.7636\n        3          96.5263       93.9409      94.7636\n        4          96.5263       93.9409      94.7636\n        5          96.3158       94.1839      95.0642\n        6          96.2105       94.6859      94.8224\n        7          96.0000       94.9121      94.8797\n        8          95.8947       95.1871      95.3046\n        9          95.2632       94.6185      94.2199\n       10          94.7368       93.8334      94.2786\n     [1ex]                                   : Decision Tree with different MinLeaf (Gini Index)   table:nonlin  table:nonlin     MinLeaf     Train accu.   Valid accu.   Test accu.  [0.5ex] 1     97.0526       93.3505      94.2163\n        2          97.0526       93.3505      94.2163\n        3          96.8421       93.9032      94.2163\n        4          96.8421       93.9032      94.2163\n        5          96.6316       94.1462      94.5168\n        6          96.5263       94.6482      94.2750\n        7          96.3158       94.8744      94.3323\n        8          95.8947       95.1871      95.3046\n        9          95.2632       94.6185      94.2199\n       10          94.7368       93.8334      94.2786\n     [1ex]                                   : Decision Tree with different MinLeaf (Cross Entropy)   table:nonlin  table:nonlin          Method           Train accu.   Valid accu.   Test accu.  [0.5ex] Naive Bayes      87.05         83.80        83.80\n       Logistic reg           83.89         81.49        85.35\n          [1ex]                                        : Performance of Naive Bayes and Logistic Reg.   table:nonlin  table:nonlin",
            "title": "5.4 Performace comparision"
        },
        {
            "location": "/solution_hw_1/#55-decision-boundary",
            "text": "As we can see in the figure when the value of K increases, the decision\nboundary smoothen out. As we can see in case of K = 20, very less red\ndata points lying in the blue region and just one chunk of blue point\nlying on left bottom of the figure.",
            "title": "5.5 Decision Boundary"
        }
    ]
}